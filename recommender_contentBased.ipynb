{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "\n",
    "from scipy.sparse import hstack, vstack, csr_matrix, load_npz, save_npz\n",
    "\n",
    "pd.set_option('display.width', 1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "STOPWORDS = list(nltk.corpus.stopwords.words('english'))\n",
    "STEMMER = nltk.stem.snowball.SnowballStemmer('english')\n",
    "LEMMATIZER = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def text_preprocessor(document):\n",
    "    tokens = []\n",
    "    for sentence in sent_tokenize(document.lower()):\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [ word for word in words if (word.isalpha() and word not in STOPWORDS) ]\n",
    "        words = [ STEMMER.stem(word) for word in words ]\n",
    "        words = [ LEMMATIZER.lemmatize(word, pos=\"v\") for word in words ]\n",
    "        tokens.extend(words)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test_data(df):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df_exploded = df.explode(['rated_recipes', 'rating_list'])\n",
    "    df_train_exploded, df_test_exploded = train_test_split(df_exploded, test_size=0.2, random_state=42)\n",
    "    df_train = df_train_exploded.groupby(level=0).agg(list).drop('ingredients', axis=1)\n",
    "    df_test = df_test_exploded.groupby(level=0).agg(list).drop('ingredients', axis=1)\n",
    "    all_user_ids = df.index\n",
    "    df_train = df_train.reindex(all_user_ids, fill_value=[]) # Re-index to ensure all user_ids are included\n",
    "    df_test = df_test.reindex(all_user_ids, fill_value=[])\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text, line_width=120):\n",
    "    lines = []\n",
    "    line = ''\n",
    "    for word in text.split():\n",
    "        if len(line) == 0:\n",
    "            line = word\n",
    "        elif len(line + ' ' + word) > line_width:\n",
    "            lines.append(line)\n",
    "            line = ''\n",
    "        else:\n",
    "            line += ' ' + word\n",
    "    lines.append(line)\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recipes & recipe_reviews\n",
    "converters = { k: literal_eval for k in ['tags', 'ingredients', 'steps', 'nutrition'] } # for evaluating strings as arrays (eg. tags)\n",
    "df_recipes = pd.read_csv('dataset/RAW_recipes.csv', converters=converters, index_col='id')\n",
    "df_recipe_reviews = pd.read_csv('dataset/Recipe_Reviews.csv', index_col='id')\n",
    "# df_interact = pd.read_csv('dataset/RAW_interactions.csv', dtype={'review': str})\n",
    "\n",
    "# converters = { k: literal_eval for k in ['rated_recipes', 'ingredients', 'rating_list'] }\n",
    "# df_userdata = pd.read_csv('dataset/User_Data.csv', converters=converters, index_col='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train and test userdata ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Create/Load Training and Test data\n",
    "converters = { k: literal_eval for k in ['rated_recipes', 'ingredients', 'rating_list'] }\n",
    "train_fn = 'dataset/User_Data_Train.csv'\n",
    "test_fn = 'dataset/User_Data_Test.csv'\n",
    "if os.path.exists(train_fn):\n",
    "    print('Loading train and test userdata ...')\n",
    "    df_train = pd.read_csv(train_fn, converters=converters, index_col='user_id')\n",
    "    df_test =  pd.read_csv(test_fn, converters=converters, index_col='user_id')\n",
    "else:\n",
    "    print('Reading userdata dataframe ...')\n",
    "    df_userdata = pd.read_csv('dataset/User_Data.csv', converters=converters, index_col='user_id')\n",
    "    print('Splitting userdata into training and test data ...')\n",
    "    df_train, df_test = get_train_and_test_data(df_userdata.head(None).copy())\n",
    "    df_train.to_csv(train_fn)\n",
    "    df_test.to_csv(test_fn)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate combined TFIDF matrix based on tags&ingredients, descriptions, and user reviews\n",
    "def create_tfidf_matrix(df_recipes: pd.DataFrame, df_recipe_reviews: pd.DataFrame, size_limit=None):\n",
    "    recipes_dataframe = df_recipes.head(size_limit).copy()\n",
    "    recipe_reviews_dataframe = df_recipe_reviews.head(size_limit).copy()\n",
    "\n",
    "    # Generate TFIDF matrix on tags and ingredients\n",
    "    print('Generating keywords TFIDF model ... ', end='')\n",
    "    start = time.time()\n",
    "    tags_corpus = [ ' '.join(x).replace('-', '') for x in recipes_dataframe['tags'].values ]\n",
    "    ingr_corpus = [ ' '.join(x)                  for x in recipes_dataframe['ingredients'].values ]\n",
    "    keywords_corpus = [ f'{tag} {ing}' for tag, ing in zip(tags_corpus, ingr_corpus) ]\n",
    "    keywords_tfidf = TfidfVectorizer()\n",
    "    keywords_tfidf_matrix = keywords_tfidf.fit_transform(keywords_corpus)\n",
    "    print('took {}'.format( str(timedelta(seconds=(time.time()-start)))[:-3] ))\n",
    "\n",
    "    # Generate TFIDF matrix on recipe descriptions\n",
    "    print('Generating descriptions TFIDF model ... ', end='')\n",
    "    start = time.time()\n",
    "    descriptions_corpus = list(recipes_dataframe['description'].fillna('').values)\n",
    "    desc_tfidf = TfidfVectorizer(preprocessor=text_preprocessor, ngram_range=(1, 2))\n",
    "    desc_tfidf_matrix = desc_tfidf.fit_transform(descriptions_corpus)\n",
    "    print('took {}'.format( str(timedelta(seconds=(time.time()-start)))[:-3] ))\n",
    "\n",
    "    # Generate TFIDF matrix on recipe reviews\n",
    "    print('Generating reviews TFIDF model ... ', end='')\n",
    "    start = time.time()\n",
    "    reviews_corpus = list(recipe_reviews_dataframe['reviews'].fillna('').values)\n",
    "    reviews_tfidf = TfidfVectorizer(preprocessor=text_preprocessor) # Only unigrams for lost computational cost\n",
    "    reviews_tfidf_matrix = reviews_tfidf.fit_transform(reviews_corpus)\n",
    "    print('took {}'.format( str(timedelta(seconds=(time.time()-start)))[:-3] ))\n",
    "\n",
    "    # Combine to one matrix which represents all item profiles\n",
    "    tfidf_matrix = hstack([keywords_tfidf_matrix, desc_tfidf_matrix, reviews_tfidf_matrix], format='csr')\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use vector at target_i from matrix\n",
    "def get_similar_items_to_index_Cosine(matrix, target_i, top_n=10):\n",
    "    target_vector = matrix[target_i]\n",
    "    cosine_sims = cosine_similarity(target_vector, matrix)[0]\n",
    "    sims_items = [ (i, sim) for i, sim in enumerate(cosine_sims) ]\n",
    "    sims_items.sort(reverse=True, key=lambda item: item[1])\n",
    "    return sims_items[1:top_n+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to target_vector\n",
    "def get_similar_items_to_vector_Cosine(matrix, target_vector, top_n=10):\n",
    "    cosine_sims = cosine_similarity(target_vector, matrix)[0]\n",
    "    sims_items = [ (i, sim) for i, sim in enumerate(cosine_sims) ]\n",
    "    sims_items.sort(reverse=True, key=lambda item: item[1])\n",
    "    return sims_items[0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_profile_matrix(tfidf_matrix: csr_matrix, training_data: pd.DataFrame, id_to_index_RECIPES: dict, user_profile_matrix_existing: csr_matrix=None, size_limit=None):\n",
    "    user_profile_matrix = []\n",
    "    if user_profile_matrix_existing != None:\n",
    "        print('Appending {:_} rows of existing data ...'.format(user_profile_matrix_existing.shape[0]))\n",
    "        for row in user_profile_matrix_existing:\n",
    "            user_profile_matrix.append(row)\n",
    "    n_rows = len(training_data)\n",
    "    for IDX, (_, row) in enumerate(training_data.head(size_limit).iterrows()):\n",
    "        if IDX >= len(user_profile_matrix):\n",
    "            print('\\rGenerating user profile for user {:_}/{:_} ({:.1f}%)'.format(IDX, n_rows, ((IDX+1)/n_rows)*100), end='')\n",
    "            rated_recipes, rating_list = row['rated_recipes'], row['rating_list']\n",
    "            rated_recipes_indices = [ id_to_index_RECIPES[id_] for id_ in rated_recipes if id_ in id_to_index_RECIPES ]\n",
    "            user_items = [ csr_matrix(tfidf_matrix[idx].multiply(rating-2)) for idx, rating in zip(rated_recipes_indices, rating_list) ] # get scaled item profiles for rated recipes\n",
    "            if user_items != []:\n",
    "                stacked_item_profiles = vstack(user_items) # Stack vertically in sparce matrix\n",
    "                user_profile = stacked_item_profiles.mean(axis=0) # Get row-wise mean\n",
    "                user_profile_matrix.append(csr_matrix(user_profile))\n",
    "            else:\n",
    "                user_profile_matrix.append(csr_matrix((1, tfidf_matrix.shape[1])))\n",
    "    print()\n",
    "    return vstack(user_profile_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_index_RECIPES = { id_: i for i, id_ in enumerate(df_recipes.index) }\n",
    "index_to_id_RECIPES = { i: id_ for i, id_ in enumerate(df_recipes.index) }\n",
    "\n",
    "id_to_index_USERS = { id_: i for i, id_ in enumerate(df_train.index) }\n",
    "index_to_id_USERS = { i: id_ for i, id_ in enumerate(df_train.index) }\n",
    "\n",
    "embeddings_savedir = 'dataset/embeddings-contentBased'\n",
    "os.makedirs(embeddings_savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TFIDF matrix ...\n"
     ]
    }
   ],
   "source": [
    "# Create/Load TFIDF matrix (recipe embeddings)\n",
    "tfidf_matrix_fn = f'{embeddings_savedir}/tfidf_matrix.npz'\n",
    "if os.path.exists(tfidf_matrix_fn):\n",
    "    print('Loading TFIDF matrix ...')\n",
    "    tfidf_matrix = load_npz(tfidf_matrix_fn)\n",
    "else:\n",
    "    print('Creating TFIDF matrix ...')\n",
    "    tfidf_matrix = create_tfidf_matrix(df_recipes, df_recipe_reviews)\n",
    "    save_npz(tfidf_matrix_fn, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING: Get similar recipes\n",
    "target_i = 425\n",
    "target_recipe = df_recipes.iloc[target_i]\n",
    "top_results = get_similar_items_to_index_Cosine(tfidf_matrix, target_i, top_n=5)\n",
    "print('TARGET RECIPE:')\n",
    "print('\"{}\" (index: {:_}):'.format(target_recipe['name'].replace('  ', ' - ').title(), target_i))\n",
    "print(format_text(target_recipe['description']))\n",
    "print('\\nRECOMMENDATIONS:')\n",
    "for j, (i, sim) in enumerate(top_results):\n",
    "    recipe = df_recipes.iloc[i]\n",
    "    print('\\n  {:>2}: SIM: {:.3f}   NAME: \"{}\"  (index: {:_})'.format( j+1, sim, recipe['name'].replace('  ', ' - ').title(), i))\n",
    "    print(format_text(recipe['description']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate/Load User Profile Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING: generating user profile for one user\n",
    "user_id = 1533\n",
    "userdata_train = df_train.loc[user_id]\n",
    "\n",
    "rated_recipes = userdata_train['rated_recipes']\n",
    "rating_list = userdata_train['rating_list']\n",
    "rated_recipes_indices = [ id_to_index_RECIPES[id_] for id_ in rated_recipes if id_ in id_to_index_RECIPES ]\n",
    "\n",
    "print('Found {} rated recipes'.format(len(rated_recipes)))\n",
    "\n",
    "print('Extracting and scaling user items ...')\n",
    "user_items = [ tfidf_matrix[idx].multiply(rating-2) for idx, rating in zip(rated_recipes_indices, rating_list) ]\n",
    "\n",
    "print('Calculating user profile ...')\n",
    "stacked_item_profiles = vstack(user_items) # Stack vertically in sparce matrix\n",
    "user_profile = stacked_item_profiles.mean(axis=0) # Get row-wise mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user profile matrix ...\n",
      "Generating user profile matrix ...\n",
      "Appending 226_570 rows of existing data ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load/Create User Profile Matrix\n",
    "user_profiles_fn = f'{embeddings_savedir}/user_profile_matrix.npz'\n",
    "user_profile_matrix = None\n",
    "if os.path.exists(user_profiles_fn):\n",
    "    print('Loading user profile matrix ...')\n",
    "    user_profile_matrix = load_npz(user_profiles_fn)\n",
    "print('Generating user profile matrix ...')\n",
    "if False:\n",
    "    user_profile_matrix = create_user_profile_matrix(tfidf_matrix, df_train, id_to_index_RECIPES, \n",
    "                                                    user_profile_matrix_existing=user_profile_matrix, size_limit=None)\n",
    "    save_npz(user_profiles_fn, user_profile_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING: User Embedding Cosine Results\n",
    "user_id = df_train.index[3]\n",
    "idx = id_to_index_USERS[user_id]\n",
    "target_vector = user_profile_matrix[idx]\n",
    "sims_indices = get_similar_items_to_vector_Cosine(tfidf_matrix, target_vector, top_n=100)\n",
    "sims_id = [ (index_to_id_RECIPES[idx], sim) for idx, sim in sims_indices ]\n",
    "recommended_recipes = [ x[0] for x in sims_id ]\n",
    "\n",
    "userdata_train = df_train.loc[user_id]\n",
    "userdata_test = df_test.loc[user_id]\n",
    "rated_recipes_train = { id_: r for id_, r in zip(userdata_train['rated_recipes'], userdata_train['rating_list']) }\n",
    "rated_recipes_test = { id_: r for id_, r in zip(userdata_test['rated_recipes'], userdata_test['rating_list']) }\n",
    "recipes_in_rec_train = [x for x in recommended_recipes if x in rated_recipes_train]\n",
    "recipes_in_rec_test = [x for x in recommended_recipes if (x in rated_recipes_test and rated_recipes_test[x] > 2) ]\n",
    "\n",
    "print('user_id:', user_id)\n",
    "print('amount of rated_recipes TRAIN:   ', len(rated_recipes_train))\n",
    "print('amount of rated_recipes TEST :   ', len(rated_recipes_test))\n",
    "print('amount of recommendations:       ', len(recommended_recipes))\n",
    "\n",
    "print('recipes in recommend TRAIN   :      ', len(recipes_in_rec_train))\n",
    "for i, id_ in enumerate(recipes_in_rec_train):\n",
    "    print('{:<10}: {}'.format(id_, rated_recipes_train[id_]))\n",
    "\n",
    "print('recipes in recommend TEST   :      ', len(recipes_in_rec_test))\n",
    "for i, id_ in enumerate(recipes_in_rec_test):\n",
    "    print('{:<10}: {}'.format(id_, rated_recipes_test[id_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets recommendations for all users and saves to disk\n",
    "def get_recommendations_for_all_users(user_embeddings, item_embeddings, user_ids, index_to_id_RECIPES, save_dir='recommendations', redo=False, size_limit=None, top_n=100):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for IDX, user_id in enumerate(user_ids):\n",
    "        save_fn = f'{save_dir}/{user_id}.pkl'\n",
    "        if redo or not os.path.exists(save_fn):\n",
    "            print('\\rGetting recommendations for {:_}/{:_} ({:.2f}%)'.format(IDX+1, len(user_ids), ((IDX+1)/len(user_ids)*100)), end='')\n",
    "            user_embedding = user_embeddings[IDX]\n",
    "            sims_indices = get_similar_items_to_vector_Cosine(item_embeddings, user_embedding, top_n=100)\n",
    "            recommended_recipes = [ index_to_id_RECIPES[idx] for idx, sim in sims_indices ]\n",
    "            recommended_sims = [ sim for idx, sim in sims_indices ]\n",
    "            dump_item = (recommended_recipes, recommended_sims)\n",
    "            with open(save_fn, \"wb\") as f:\n",
    "                pickle.dump(dump_item, f)\n",
    "        if size_limit and IDX >= size_limit: break\n",
    "    print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recommendations_for_all_users(save_dir='recommendations', limit=None):\n",
    "    if not os.path.exists(save_dir):\n",
    "        print('savedir doesnt exist:', save_dir)\n",
    "        return None\n",
    "    user_ids, recipes, sims = [], [], []\n",
    "    n_items = len(os.listdir(save_dir))\n",
    "    for i, item in enumerate(os.listdir(save_dir)):\n",
    "        print('\\rLoading {:_}/{:_}'.format(i+1, n_items), end='')\n",
    "        user_id = item.split('.')[0]\n",
    "        itempath = os.path.join(save_dir, item)\n",
    "        try:\n",
    "            with open(itempath, 'rb') as f:\n",
    "                recommended_recipes, recommended_sims = pickle.load(f)\n",
    "            user_ids.append(user_id)\n",
    "            recipes.append(recommended_recipes)\n",
    "            sims.append(recommended_sims)\n",
    "        except:\n",
    "            print('\\nError: Unable to read \"{}\"'.format(itempath))\n",
    "        if limit and i >= limit: break\n",
    "    print('\\nDone.')\n",
    "    df_recommend = pd.DataFrame({'user_id': user_ids, 'recommended_recipes': recipes, 'recommended_sims': sims})\n",
    "    df_recommend = df_recommend.set_index('user_id')\n",
    "    return df_recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting recommendations for 31_838/226_570 (14.05%)\n",
      "Keyboard interrupt detected ...\n"
     ]
    }
   ],
   "source": [
    "# Compute recommendations for users and save to disk\n",
    "recommend_save_dir = 'dataset/recommendations_contentBased_tfidf'\n",
    "try:\n",
    "    get_recommendations_for_all_users(\n",
    "        user_profile_matrix, tfidf_matrix, df_train.index, index_to_id_RECIPES, save_dir=recommend_save_dir, top_n=100_000\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print('\\nKeyboard interrupt detected ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user recommendations from disk\n",
    "try:\n",
    "    df_recommend = load_recommendations_for_all_users(recommend_save_dir)\n",
    "    df_recommend\n",
    "except KeyboardInterrupt:\n",
    "    print('\\nKeyboard interrupt detected ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommender Evaluation Metrics\n",
    "def get_precision(recommended, liked):\n",
    "    k = len(recommended)\n",
    "    true_positives = [ x for x in recommended if x in liked ]\n",
    "    return len(true_positives) / k\n",
    "\n",
    "def get_recall(recommended, liked):\n",
    "    true_positives = [ x for x in recommended if x in liked ]\n",
    "    return len(true_positives) / len(liked)\n",
    "\n",
    "def get_f_score(recommended, liked, beta=1):\n",
    "    precision = get_precision(recommended, liked)\n",
    "    recall = get_recall(recommended, liked)\n",
    "    return (1 + beta**2) * (precision * recall) / ((precision*beta**2) + recall)\n",
    "\n",
    "def get_mean_average_precision(recommended, liked):\n",
    "    sum = 0\n",
    "    true_positives = [ x for x in recommended if x in liked ]\n",
    "    for i in range(1, len(recommended)):\n",
    "        Slice = recommended[:i]\n",
    "        sum += get_precision(Slice, liked)\n",
    "    return len(true_positives) * sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test recipes and their ratings\n",
    "i = 0\n",
    "userdata_test = df_test.iloc[i]\n",
    "rated_recipes_test = userdata_test['rated_recipes']\n",
    "rating_list_test = userdata_test['rating_list']\n",
    "\n",
    "for recipe_id, rating in zip(rated_recipes_test, rating_list_test):\n",
    "    similar_users_who_rated = [ idx for idx, row in df_train.iterrows() if recipe_id in row['rated_recipes'] ]\n",
    "    print(similar_users_who_rated)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csr matrix\n",
    "data, rows, cols = [], [], []\n",
    "for i, (user_id, row) in enumerate(df_train.iterrows()):\n",
    "    for recipe_id, rating in zip(row['rated_recipes'], row['rating_list']):\n",
    "        user_IDX = id_to_index_USERS[user_id]\n",
    "        recipe_IDX = id_to_index_RECIPES[recipe_id]\n",
    "        data.append(rating)\n",
    "        rows.append(user_IDX)\n",
    "        cols.append(recipe_IDX)\n",
    "user_item_matrix = csr_matrix((data, (rows, cols)), shape=(len(df_train), len(df_recipes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix[0]\n",
    "np.corrcoef([1,2,4,3], [1,2,2,3])[0][1]\n",
    "i = 0\n",
    "user_vector = user_item_matrix[i].toarray()\n",
    "ccs = []\n",
    "for j, sp_vector in enumerate(user_item_matrix):\n",
    "    # vector = user_item_matrix[j].toarray()\n",
    "    cc = np.corrcoef(user_vector, sp_vector.toarray())[0][1]\n",
    "    ccs.append(cc)\n",
    "ccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, vector in enumerate(user_item_matrix):\n",
    "    print(vector.toarray())\n",
    "    if i > 5: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
